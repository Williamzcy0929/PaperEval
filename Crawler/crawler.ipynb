{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Necessary Libraries Before Running the Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requests\n",
    "# pip install pymupdf\n",
    "# pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import requests  # For downloading PDF\n",
    "import fitz      # PyMuPDF for PDF text extraction\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFERENCE_URL = \"https://openreview.net/...\" #change this to the conference URL\n",
    "OUTPUT_FILE = \"output.json\" #change this to the output file name\n",
    "ERROR_FILE = \"errors.json\" #change this to the error file name\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.headless = True\n",
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_data = []\n",
    "errors_data = []\n",
    "processed_ids = set()\n",
    "\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    try:\n",
    "        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            existing = json.load(f)\n",
    "        if isinstance(existing, list):\n",
    "            papers_data = existing\n",
    "            for p in papers_data:\n",
    "                if \"id\" in p:\n",
    "                    processed_ids.add(p[\"id\"])\n",
    "        print(f\"[INFO] Loaded {len(processed_ids)} papers from {OUTPUT_FILE}, will skip these.\")\n",
    "    except (json.JSONDecodeError, OSError):\n",
    "        print(\"[WARN] Could not parse existing success file. Starting fresh.\")\n",
    "        papers_data = []\n",
    "        processed_ids = set()\n",
    "\n",
    "if os.path.exists(ERROR_FILE):\n",
    "    try:\n",
    "        with open(ERROR_FILE, 'r', encoding='utf-8') as f:\n",
    "            existing_errors = json.load(f)\n",
    "        if isinstance(existing_errors, list):\n",
    "            errors_data = existing_errors\n",
    "    except (json.JSONDecodeError, OSError):\n",
    "        print(\"[WARN] Could not parse existing error file. Starting fresh for errors.\")\n",
    "        errors_data = []\n",
    "else:\n",
    "    errors_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n[INFO] Opening main tab page: {CONFERENCE_URL}\")\n",
    "driver.get(CONFERENCE_URL)\n",
    "\n",
    "try:\n",
    "    WebDriverWait(driver, 15).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//a[contains(@href, '/forum?id=')]\"))\n",
    "    )\n",
    "except Exception:\n",
    "    print(\"[ERROR] Forum links did not load; quitting.\")\n",
    "    driver.quit()\n",
    "    raise\n",
    "\n",
    "link_elems = driver.find_elements(By.XPATH, \"//a[contains(@href, '/forum?id=')]\")\n",
    "scroll_attempts = 0\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollBy(0, 1000);\")\n",
    "    time.sleep(2)\n",
    "    new_elems = driver.find_elements(By.XPATH, \"//a[contains(@href, '/forum?id=')]\")\n",
    "    if len(new_elems) > len(link_elems):\n",
    "        link_elems = new_elems\n",
    "        scroll_attempts = 0\n",
    "    else:\n",
    "        scroll_attempts += 1\n",
    "        if scroll_attempts > 3:\n",
    "            break\n",
    "\n",
    "forum_urls = []\n",
    "seen_forum_ids = set()\n",
    "for elem in link_elems:\n",
    "    href = elem.get_attribute('href')\n",
    "    if href and \"/forum?id=\" in href:\n",
    "        fid = href.split(\"id=\")[-1]\n",
    "        if fid not in seen_forum_ids:\n",
    "            seen_forum_ids.add(fid)\n",
    "            forum_urls.append(href)\n",
    "\n",
    "print(f\"[INFO] Found {len(forum_urls)} unique forum links.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Download and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(pdf_url, output_path=\"temp.pdf\"):\n",
    "    \"\"\"\n",
    "    Downloads a PDF from pdf_url, saving it locally to output_path.\n",
    "    Returns the path to the saved file.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Downloading PDF from: {pdf_url}\")\n",
    "    response = requests.get(pdf_url)\n",
    "    response.raise_for_status()  # if status != 200, raise an HTTPError\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"[INFO] Saved PDF to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Opens the local PDF using PyMuPDF and extracts text from all pages.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Extracting text from PDF: {pdf_path}\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "    for page in doc:\n",
    "        page_text = page.get_text(\"text\")\n",
    "        all_text.append(page_text)\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def extract_emails_from_text(text):\n",
    "    \"\"\"\n",
    "    Handles both standard emails and bracketed variants like {name1, name2}@domain.com.\n",
    "    Splits local parts by commas, then appends the domain.\n",
    "    Returns a list of all extracted emails (or an empty list).\n",
    "    \"\"\"\n",
    "    # Pattern for bracketed local parts\n",
    "    bracketed_pat = r\"\\{([^}@]+)\\}@([a-zA-Z0-9.\\-]+\\.[a-zA-Z]{2,})\"\n",
    "    # Pattern for standard addresses\n",
    "    standard_pat  = r\"\\b[a-zA-Z0-9._%+\\-]+@[a-zA-Z0-9.\\-]+\\.[a-zA-Z]{2,}\\b\"\n",
    "\n",
    "    found_emails = []\n",
    "\n",
    "    # 1) bracketed pattern matches, e.g. {user1, user2}@domain.com\n",
    "    bracketed = re.findall(bracketed_pat, text)\n",
    "    for local_parts, domain in bracketed:\n",
    "        # e.g. \"user1, user2\", \"example.com\"\n",
    "        sub_parts = [x.strip() for x in local_parts.split(',')]\n",
    "        for sub in sub_parts:\n",
    "            found_emails.append(f\"{sub}@{domain}\")\n",
    "\n",
    "    # 2) standard pattern matches, e.g. user@domain.com\n",
    "    standard = re.findall(standard_pat, text)\n",
    "    found_emails.extend(standard)\n",
    "\n",
    "    # Optionally deduplicate results\n",
    "    found_emails = list(set(found_emails))\n",
    "    return found_emails\n",
    "\n",
    "def extract_emails_from_pdf(pdf_url, output_path=\"temp.pdf\"):\n",
    "    \"\"\"\n",
    "    1. Download the PDF from pdf_url to output_path.\n",
    "    2. Extract text with PyMuPDF.\n",
    "    3. Parse all email addresses (bracketed and standard).\n",
    "    4. Remove the local PDF file whether or not emails are found.\n",
    "    5. Return a list of emails or an empty list if none exist.\n",
    "    \"\"\"\n",
    "    # Download\n",
    "    local_path = download_pdf(pdf_url, output_path)\n",
    "    try:\n",
    "        # Extract text\n",
    "        pdf_text = extract_text_from_pdf(local_path)\n",
    "        # Extract emails\n",
    "        email_list = extract_emails_from_text(pdf_text)\n",
    "        return email_list\n",
    "    finally:\n",
    "        # Remove PDF file after finished\n",
    "        if os.path.exists(local_path):\n",
    "            os.remove(local_path)\n",
    "            print(f\"[INFO] Deleted temporary PDF: {local_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Author Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_author_profile(profile_url):\n",
    "    \"\"\"\n",
    "    Opens the author's profile in a new tab, extracts name and email domain.\n",
    "    \"\"\"\n",
    "    profile_data = {\"name\": None, \"id\": None, \"email_domain\": None}\n",
    "    if not profile_url:\n",
    "        return profile_data\n",
    "\n",
    "    try:\n",
    "        author_id = profile_url.split(\"id=\")[-1]\n",
    "        profile_data[\"id\"] = author_id\n",
    "\n",
    "        main_window = driver.current_window_handle\n",
    "        driver.execute_script(\"window.open(arguments[0], '_blank');\", profile_url)\n",
    "        WebDriverWait(driver, 10).until(lambda d: len(d.window_handles) == 2)\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "        print(f\"[INFO] Scraping author profile: {profile_url}\")\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "        try:\n",
    "            h1_elem = driver.find_element(By.TAG_NAME, 'h1')\n",
    "            profile_data[\"name\"] = h1_elem.text.strip()\n",
    "        except NoSuchElementException:\n",
    "            try:\n",
    "                h2_elem = driver.find_element(By.TAG_NAME, 'h2')\n",
    "                profile_data[\"name\"] = h2_elem.text.strip()\n",
    "            except NoSuchElementException:\n",
    "                profile_data[\"name\"] = None\n",
    "\n",
    "        page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "        email_match = re.search(r'@([\\w\\.-]+\\.\\w+)\\s+\\(Confirmed\\)', page_text)\n",
    "        if not email_match:\n",
    "            email_match = re.search(r'@([\\w\\.-]+\\.\\w+)', page_text)\n",
    "        if email_match:\n",
    "            profile_data[\"email_domain\"] = email_match.group(1)\n",
    "\n",
    "        print(f\"[INFO] Author name: {profile_data['name']}, domain: {profile_data['email_domain']}\")\n",
    "    except Exception as e:\n",
    "        profile_data[\"error\"] = f\"Author profile error: {str(e)}\"\n",
    "        print(f\"[WARN] Could not scrape author profile: {e}\")\n",
    "    finally:\n",
    "        if len(driver.window_handles) == 2:\n",
    "            driver.close()\n",
    "        driver.switch_to.window(main_window)\n",
    "\n",
    "    return profile_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Official Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_official_review_text(text_block):\n",
    "    review_dict = {\n",
    "        \"summary\": \"\",\n",
    "        \"strengths\": \"\",\n",
    "        \"weaknesses\": \"\",\n",
    "        \"questions\": \"\",\n",
    "        \"ethics\": \"\",\n",
    "        \"rating\": \"\",\n",
    "        \"confidence\": \"\",\n",
    "        \"code_of_conduct\": \"\"\n",
    "    }\n",
    "    lines = text_block.splitlines()\n",
    "\n",
    "    if lines and \"Official Review of Submission\" in lines[0]:\n",
    "        lines = lines[1:]\n",
    "\n",
    "    current_field = None\n",
    "    headings_map = {\n",
    "        \"summary\": \"summary\",\n",
    "        \"strengths\": \"strengths\",\n",
    "        \"weaknesses\": \"weaknesses\",\n",
    "        \"questions\": \"questions\",\n",
    "        \"flag for ethics review\": \"ethics\",\n",
    "        \"rating\": \"rating\",\n",
    "        \"confidence\": \"confidence\",\n",
    "        \"code of conduct\": \"code_of_conduct\"\n",
    "    }\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        lower_line = stripped.lower()\n",
    "\n",
    "        matched_heading = None\n",
    "        for heading_text, key in headings_map.items():\n",
    "            if lower_line.startswith(heading_text):\n",
    "                matched_heading = key\n",
    "                colon_idx = stripped.find(\":\")\n",
    "                if colon_idx != -1:\n",
    "                    remainder = stripped[colon_idx + 1:].strip()\n",
    "                    review_dict[key] = remainder\n",
    "                else:\n",
    "                    review_dict[key] = \"\"\n",
    "                break\n",
    "\n",
    "        if matched_heading:\n",
    "            current_field = matched_heading\n",
    "        else:\n",
    "            if current_field:\n",
    "                review_dict[current_field] += \" \" + stripped\n",
    "\n",
    "    return review_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Core Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paper_data(forum_url):\n",
    "    \"\"\"\n",
    "    Scrape a single paper's data, returning a dict.\n",
    "    Also downloads the PDF if available and parses emails from it.\n",
    "    \"\"\"\n",
    "    paper_info = {\n",
    "        \"id\": forum_url.split(\"id=\")[-1],\n",
    "        \"url\": forum_url,\n",
    "        \"title\": None,\n",
    "        \"abstract\": None,\n",
    "        \"decision\": None,\n",
    "        \"first_author\": None,\n",
    "        \"second_author\": None,\n",
    "        \"last_author\": None,\n",
    "        \"reviews\": [],\n",
    "        \"pdf_link\": None,\n",
    "        \"pdf_emails\": []\n",
    "    }\n",
    "\n",
    "    print(f\"[INFO] Navigating to forum: {forum_url}\")\n",
    "    driver.get(forum_url)\n",
    "\n",
    "    # Wait for the forum content or fallback\n",
    "    try:\n",
    "        loading_elem = driver.find_element(By.XPATH, \"//*[text()='Loading']\")\n",
    "        WebDriverWait(driver, 15).until(EC.staleness_of(loading_elem))\n",
    "    except NoSuchElementException:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//*[contains(text(), 'Abstract') or contains(text(),'Official Review')]\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Timeout waiting for forum content. {e}\")\n",
    "\n",
    "    # Scrape title\n",
    "    print(\"[INFO] Scraping title...\")\n",
    "    try:\n",
    "        title_elem = driver.find_element(By.TAG_NAME, 'h2')\n",
    "        title_text = title_elem.text.strip()\n",
    "        if \"Download PDF\" in title_text:\n",
    "            title_text = title_text.split(\"[\")[0].strip()\n",
    "        paper_info[\"title\"] = title_text\n",
    "    except Exception as e:\n",
    "        paper_info[\"title\"] = None\n",
    "        raise Exception(f\"Failed to extract title: {e}\")\n",
    "\n",
    "    # Scrape abstract\n",
    "    print(\"[INFO] Scraping abstract...\")\n",
    "    abstract_text = \"\"\n",
    "    try:\n",
    "        abstract_label = driver.find_element(\n",
    "            By.XPATH,\n",
    "            \"//*[contains(translate(text(),'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'abstract')]\"\n",
    "        )\n",
    "        parent_text = abstract_label.find_element(By.XPATH, \"..\").text\n",
    "        abstract_text = parent_text.replace(\"Abstract:\", \"\").replace(\"ABSTRACT:\", \"\").strip()\n",
    "    except NoSuchElementException:\n",
    "        print(\"[WARN] Could not locate an element containing 'Abstract'. Abstract set to None.\")\n",
    "    paper_info[\"abstract\"] = abstract_text\n",
    "\n",
    "    # Scrape decision\n",
    "    print(\"[INFO] Scraping decision...\")\n",
    "    try:\n",
    "        body_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "        decision_match = re.search(r'Decision:\\s*(.+)', body_text)\n",
    "        if decision_match:\n",
    "            decision_str = decision_match.group(1).split('\\n')[0].strip()\n",
    "            paper_info[\"decision\"] = decision_str\n",
    "        else:\n",
    "            pattern = re.search(r'Accept\\s*\\([\\w\\s]+\\)|Reject|Withdrawn', body_text)\n",
    "            if pattern:\n",
    "                paper_info[\"decision\"] = pattern.group(0)\n",
    "            else:\n",
    "                paper_info[\"decision\"] = None\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not parse decision: {e}\")\n",
    "        paper_info[\"decision\"] = None\n",
    "\n",
    "    # Scrape authors\n",
    "    print(\"[INFO] Scraping authors...\")\n",
    "    author_elems = driver.find_elements(By.XPATH, \"//h3//a\")\n",
    "    author_urls = [elem.get_attribute('href') for elem in author_elems]\n",
    "    num_authors = len(author_urls)\n",
    "\n",
    "    if num_authors >= 1:\n",
    "        paper_info[\"first_author\"] = scrape_author_profile(author_urls[0])\n",
    "    if num_authors >= 2:\n",
    "        paper_info[\"second_author\"] = scrape_author_profile(author_urls[1])\n",
    "    if num_authors == 1:\n",
    "        paper_info[\"last_author\"] = paper_info[\"first_author\"]\n",
    "    elif num_authors == 2:\n",
    "        paper_info[\"last_author\"] = paper_info[\"second_author\"]\n",
    "    elif num_authors >= 3:\n",
    "        paper_info[\"last_author\"] = scrape_author_profile(author_urls[-1])\n",
    "\n",
    "    # Scrape PDF link\n",
    "    print(\"[INFO] Scraping PDF link...\")\n",
    "    try:\n",
    "        pdf_elem = driver.find_element(By.XPATH, \"//a[@class='citation_pdf_url']\")\n",
    "        pdf_link = pdf_elem.get_attribute(\"href\")\n",
    "        paper_info[\"pdf_link\"] = pdf_link\n",
    "    except NoSuchElementException:\n",
    "        paper_info[\"pdf_link\"] = None\n",
    "\n",
    "    # If PDF link found, download PDF and extract emails\n",
    "    if paper_info[\"pdf_link\"]:\n",
    "        try:\n",
    "            print(\"[INFO] Downloading and parsing PDF for emails...\")\n",
    "            local_pdf_path = download_pdf(paper_info[\"pdf_link\"], \"temp_openreview.pdf\")\n",
    "            pdf_text = extract_text_from_pdf(local_pdf_path)\n",
    "            emails_in_pdf = extract_emails_from_text(pdf_text)\n",
    "            paper_info[\"pdf_emails\"] = emails_in_pdf\n",
    "\n",
    "            # Optionally remove the PDF after finishing\n",
    "            # os.remove(local_pdf_path)\n",
    "\n",
    "            print(f\"[INFO] Found {len(emails_in_pdf)} email(s) in PDF.\")\n",
    "\n",
    "            # Attach an email to first, second, and last author if needed:\n",
    "            if emails_in_pdf:\n",
    "                # first author gets the first email\n",
    "                if paper_info[\"first_author\"]:\n",
    "                    paper_info[\"first_author\"][\"full_email\"] = emails_in_pdf[0]\n",
    "\n",
    "                # second author gets the second if available\n",
    "                if len(emails_in_pdf) >= 2 and paper_info[\"second_author\"]:\n",
    "                    paper_info[\"second_author\"][\"full_email\"] = emails_in_pdf[1]\n",
    "\n",
    "                # last author gets the last in the list if we have 3 or more\n",
    "                if len(emails_in_pdf) >= 3 and paper_info[\"last_author\"]:\n",
    "                    paper_info[\"last_author\"][\"full_email\"] = emails_in_pdf[-1]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not download/parse PDF: {e}\")\n",
    "\n",
    "    # Scrape reviews\n",
    "    print(\"[INFO] Scraping reviews...\")\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((\n",
    "                By.XPATH,\n",
    "                \"//h4/span[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), 'official review of submission')]\"\n",
    "            ))\n",
    "        )\n",
    "    except Exception:\n",
    "        print(\"[WARN] No 'Official Review of Submission' found after 10 seconds.\")\n",
    "        page_html = driver.page_source\n",
    "        print(\"[DEBUG] Partial Page Source (first 2000 chars):\")\n",
    "        print(page_html[:2000])\n",
    "        all_spans = driver.find_elements(By.XPATH, \"//h4/span\")\n",
    "        print(f\"[DEBUG] Found {len(all_spans)} <h4><span> elements. Listing their text:\")\n",
    "        for i, sp in enumerate(all_spans, start=1):\n",
    "            print(f\"  Span #{i} text = {repr(sp.text)}\")\n",
    "    else:\n",
    "        review_blocks = driver.find_elements(\n",
    "            By.XPATH,\n",
    "            \"//h4/span[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), 'official review of submission')]/../../following-sibling::div[contains(@class,'note-content') or contains(@class,'note-content-container')]\"\n",
    "        )\n",
    "        for block in review_blocks:\n",
    "            review_text_block = block.text.strip()\n",
    "            parsed_review = parse_official_review_text(review_text_block)\n",
    "            reviews_data.append(parsed_review)\n",
    "\n",
    "    paper_info[\"reviews\"] = reviews_data\n",
    "\n",
    "    print(\"[INFO] Finished scraping this paper.\")\n",
    "    return paper_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_list = []\n",
    "\n",
    "print(f\"[INFO] Found {len(forum_urls)} forum links. Beginning scraping...\")\n",
    "\n",
    "for i, link in enumerate(forum_urls, start=1):\n",
    "    fid = link.split('id=')[-1]\n",
    "    if fid in processed_ids:\n",
    "        print(f\"[{i}/{len(forum_urls)}] Skipping already processed paper (Forum ID={fid}).\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n[{i}/{len(forum_urls)}] Processing forum: {fid}\")\n",
    "    partial_data = {\"id\": fid, \"url\": link}\n",
    "\n",
    "    try:\n",
    "        paper_data = extract_paper_data(link)\n",
    "        papers_data.append(paper_data)\n",
    "        processed_ids.add(fid)\n",
    "\n",
    "        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(papers_data, f, indent=2)\n",
    "        print(f\"[INFO] Successfully saved data for forum {fid}.\")\n",
    "    except Exception as e:\n",
    "        partial_data[\"error_message\"] = str(e)\n",
    "        errors_list.append(partial_data)\n",
    "        print(f\"[ERROR] Encountered an error with forum {fid}: {str(e)}\")\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "errors_data.extend(errors_list)\n",
    "\n",
    "if errors_data:\n",
    "    with open(ERROR_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(errors_data, f, indent=2)\n",
    "    print(f\"[INFO] Wrote {len(errors_data)} total errors to {ERROR_FILE}.\")\n",
    "\n",
    "driver.quit()\n",
    "print(f\"\\n[INFO] All done! Processed {len(papers_data)} papers successfully, with {len(errors_data)} errors.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaperEval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
