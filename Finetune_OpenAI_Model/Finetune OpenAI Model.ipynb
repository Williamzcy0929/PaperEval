{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Necessary Libraries Before Running the Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy\n",
    "# pip install sklearn\n",
    "# pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY'] # set your API key in the environment variables\n",
    "\n",
    "# set your API key here if you don't want to use environment variables\n",
    "# openai.api_key = \"some OpenAI API key\"\n",
    "\n",
    "print(openai.api_key)\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"No OPENAI_API_KEY found in environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('paper_data_file', 'r') as f:\n",
    "with open('iclr2024.json', 'r') as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "def preprocess_data(papers):\n",
    "    \"\"\"\n",
    "    Extract metadata, reviewer scores, and comments for each paper,\n",
    "    and store them in a Python list.\n",
    "    We'll convert them to chat format later in the fine_tune_model function.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for paper in papers:\n",
    "        # Basic info\n",
    "        title = paper.get('title', '')\n",
    "        abstract = paper.get('abstract', '')\n",
    "        authors = ', '.join(paper.get('authors', []))\n",
    "\n",
    "        # Normalize acceptance decision\n",
    "        decision = paper.get('decision', '').strip()\n",
    "        label = \"Accept\" if decision.lower().startswith('accept') else \"Reject\"\n",
    "\n",
    "        # Extract reviewer info\n",
    "        reviews = paper.get('reviews', [])\n",
    "        scores = \", \".join(str(r.get('score', 'N/A')) for r in reviews) if reviews else \"N/A\"\n",
    "        comments = \" \".join(r.get('comment', '').strip() for r in reviews) if reviews else \"N/A\"\n",
    "\n",
    "        dataset.append({\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"authors\": authors,\n",
    "            \"scores\": scores,\n",
    "            \"comments\": comments,\n",
    "            \"label\": label\n",
    "        })\n",
    "    return dataset\n",
    "\n",
    "dataset = preprocess_data(papers)\n",
    "print(\"Number of papers:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Dataset into Training Set and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "print(\"Number of training samples:\", len(train_set))\n",
    "print(\"Number of test samples:\", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best feature set is \"with_comments\" based on validation results\n",
    "best_feature_set = \"with_comments\"  # e.g., \"metadata_only\", \"with_scores\", \"with_comments\"\n",
    "\n",
    "def build_user_content(entry, feature_type):\n",
    "    \"\"\"\n",
    "    Build the 'content' of the user's message depending on the chosen feature type.\n",
    "    \"\"\"\n",
    "    base_text = f\"Title: {entry['title']}\\nAbstract: {entry['abstract']}\\nAuthors: {entry['authors']}\"\n",
    "    \n",
    "    if feature_type == \"metadata_only\":\n",
    "        # Just metadata\n",
    "        user_content = base_text + \"\\nDecision:\"\n",
    "    elif feature_type == \"with_scores\":\n",
    "        # metadata + scores\n",
    "        user_content = base_text + f\"\\nReviewer Scores: {entry['scores']}\\nDecision:\"\n",
    "    else:\n",
    "        # with_comments => metadata + scores + comments\n",
    "        user_content = (\n",
    "            base_text +\n",
    "            f\"\\nReviewer Scores: {entry['scores']}\" +\n",
    "            f\"\\nReviewer Comments: {entry['comments']}\\nDecision:\"\n",
    "        )\n",
    "    \n",
    "    return user_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(train_data, feature_type):\n",
    "    \"\"\"\n",
    "    Takes the full training set and a feature type,\n",
    "    prepares a JSONL file in Chat Format, and creates a fine-tuning job.\n",
    "    The last message is from the assistant, containing the correct label.\n",
    "    \"\"\"\n",
    "\n",
    "    data_for_finetuning = []\n",
    "    for entry in train_data:\n",
    "        # Construct the user prompt\n",
    "        user_message_content = build_user_content(entry, feature_type)\n",
    "        \n",
    "        # Create a 'messages' array where the last message is from the assistant\n",
    "        chat_item = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI that predicts whether a paper should be Accept or Reject.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_message_content\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    # The final correct answer, e.g. \"Accept\" or \"Reject\"\n",
    "                    \"content\": entry[\"label\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        data_for_finetuning.append(chat_item)\n",
    "    \n",
    "    # Save to JSONL\n",
    "    jsonl_path = \"chat_fine_tune_data.jsonl\"\n",
    "    with open(jsonl_path, \"w\") as f:\n",
    "        for record in data_for_finetuning:\n",
    "            json.dump(record, f)\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    # Upload file to OpenAI\n",
    "    upload_response = openai.File.create(\n",
    "        file=open(jsonl_path, \"rb\"),\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "    file_id = upload_response[\"id\"]\n",
    "    print(f\"Uploaded file ID: {file_id}\")\n",
    "\n",
    "    # Create fine-tune job\n",
    "    fine_tune_response = openai.FineTuningJob.create(\n",
    "        training_file=file_id,\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        suffix=\"PaperEval\",\n",
    "    )\n",
    "    \n",
    "    return fine_tune_response[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fine-tuning\n",
    "fine_tune_job_id = fine_tune_model(train_set, best_feature_set)\n",
    "print(f\"Fine-Tuning Job started: {fine_tune_job_id}\")\n",
    "\n",
    "# Poll the fine-tuning job status\n",
    "poll_interval = 60  # in seconds\n",
    "while True:\n",
    "    job_status = openai.FineTuningJob.retrieve(fine_tune_job_id)\n",
    "    status = job_status[\"status\"]\n",
    "    print(f\"Current fine-tuning job status: {status}\")\n",
    "    \n",
    "    if status == \"succeeded\":\n",
    "        final_model_id = job_status[\"fine_tuned_model\"]\n",
    "        print(f\"Fine-tuning succeeded! Model ID: {final_model_id}\")\n",
    "        break\n",
    "    elif status == \"failed\":\n",
    "        raise RuntimeError(\"Fine-tuning job failed.\")\n",
    "    else:\n",
    "        time.sleep(poll_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_data, model_id, feature_type):\n",
    "    \"\"\"\n",
    "    Uses the fine-tuned chat model to predict labels on the test set,\n",
    "    then calculates Accuracy, Precision, Recall, and F1-score.\n",
    "    \"\"\"\n",
    "    test_prompts = [build_user_content(entry, feature_type) for entry in test_data]\n",
    "    true_labels = [entry[\"label\"] for entry in test_data]\n",
    "\n",
    "    predictions = []\n",
    "    for prompt in test_prompts:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI that predicts whether a paper should be Accept or Reject.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=1,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        # The chat response is in `response.choices[0].message.content`\n",
    "        pred = response['choices'][0]['message']['content'].strip()\n",
    "        pred_label = \"Accept\" if pred.lower().startswith(\"accept\") else \"Reject\"\n",
    "        predictions.append(pred_label)\n",
    "    \n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    prec = precision_score(true_labels, predictions, pos_label=\"Accept\")\n",
    "    rec = recall_score(true_labels, predictions, pos_label=\"Accept\")\n",
    "    f1 = f1_score(true_labels, predictions, pos_label=\"Accept\")\n",
    "\n",
    "    print(\"\\nEvaluation on Test Set:\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Performance (ChatCompletion Approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use this method when have some API keys permission error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_data, model_id, feature_type):\n",
    "    \"\"\"\n",
    "    Uses a fine-tuned chat model to predict labels on the test set,\n",
    "    then calculates Accuracy, Precision, Recall, and F1-score.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    test_data : list\n",
    "        A list of dictionaries, each containing paper information and a 'label'.\n",
    "    model_id : str\n",
    "        The fine-tuned model ID in the format \"ft:...\" (e.g., \"ft:gpt-4:some_id\").\n",
    "    feature_type : str\n",
    "        Determines which parts of the paper information to include in the user prompt\n",
    "        (e.g., \"metadata_only\", \"with_scores\", \"with_comments\").\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the content of the \"user\" messages for the test set\n",
    "    test_prompts = [build_user_content(entry, feature_type) for entry in test_data]\n",
    "    \n",
    "    # Get the ground-truth labels for comparison\n",
    "    true_labels = [entry[\"label\"] for entry in test_data]\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    # For each paper, call the fine-tuned model\n",
    "    for prompt in test_prompts:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_id,  # e.g., \"ft:gpt-3.5-turbo:xxxx\"\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI that predicts whether a paper should be Accept or Reject.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=1,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        # Extract the model's response from 'assistant' message\n",
    "        pred_text = response['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        # Convert the raw text to a label: \"Accept\" or \"Reject\"\n",
    "        pred_label = \"Accept\" if pred_text.lower().startswith(\"accept\") else \"Reject\"\n",
    "        predictions.append(pred_label)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    prec = precision_score(true_labels, predictions, pos_label=\"Accept\")\n",
    "    rec = recall_score(true_labels, predictions, pos_label=\"Accept\")\n",
    "    f1 = f1_score(true_labels, predictions, pos_label=\"Accept\")\n",
    "\n",
    "    # Print the final evaluation results\n",
    "    print(\"\\nEvaluation on Test Set:\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Model Information for Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the fine-tuning jobs\n",
    "jobs = openai.FineTuningJob.list()\n",
    "for job in jobs.data:\n",
    "    print(job)\n",
    "\n",
    "# print the fine-tuning job details for debugging\n",
    "job_id = \"some fine-tuning job ID\"\n",
    "try:\n",
    "    job_info = openai.FineTuningJob.retrieve(job_id)\n",
    "    print(\"Found job:\", job_info)\n",
    "except openai.error.PermissionError:\n",
    "    print(\"No permission to access this Fine-Tuning Job.\")\n",
    "\n",
    "# print the models can be accessed\n",
    "models = openai.Model.list()\n",
    "model_ids = [m[\"id\"] for m in models[\"data\"]]\n",
    "print(\"Models you can access:\", model_ids)\n",
    "\n",
    "# print the fine-tuned model details\n",
    "model_info = openai.Model.retrieve(\"some fine-tuned model ID\")\n",
    "print(model_info)\n",
    "\n",
    "# print the fine-tuned model details for debugging\n",
    "model_id = \"some fine-tuned model ID\"\n",
    "try:\n",
    "    model_info = openai.Model.retrieve(model_id)\n",
    "    print(\"Found model:\", model_info)\n",
    "except openai.error.PermissionError:\n",
    "    print(\"No permission to access this model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_id = \"some fine-tuned model ID\"  # change this to the final model ID\n",
    "print(\"Final model ID:\", final_model_id)\n",
    "evaluate_model(test_set, final_model_id, best_feature_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaperEval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
